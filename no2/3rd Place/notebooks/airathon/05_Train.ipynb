{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4466621d",
   "metadata": {},
   "source": [
    "<h1><center> NASA Airathon - NO2 Track </center></h1>\n",
    "\n",
    "### <center> Training: Light GBM </center>\n",
    "\n",
    "<div style=\"text-align: center\"> \n",
    "    Dr. Sukanta Basu <br/> Associate Professor <br/> Delft University of Technology, The Netherlands <br/> Email: s.basu@tudelft.nl<br/> https://sites.google.com/view/sukantabasu/\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf19662",
   "metadata": {},
   "source": [
    "#### Log\n",
    "\n",
    "Last updated: 4th April, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35acb3",
   "metadata": {},
   "source": [
    "#### User instructions\n",
    "\n",
    "Run this notebook twice. First with trnOpt = 1 followed by trnOpt = 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9359d",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pickle import dump, load\n",
    "import time\n",
    "\n",
    "#For reproducibility of the results, the following seeds should be selected \n",
    "from numpy.random import seed\n",
    "seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341b750",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a30b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR    = '../../'\n",
    "\n",
    "#Location of processed datasets\n",
    "EXTDATA_DIR = ROOT_DIR + 'data/airathon/processed/'\n",
    "\n",
    "#Location of saved models\n",
    "TUNING_DIR  = ROOT_DIR + 'model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552a6e0",
   "metadata": {},
   "source": [
    "#### User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrial = 256 #required input for halving random grid search\n",
    "nEns   = 100 #number of ensembles\n",
    "\n",
    "trnOpt  = 2 #1: OBS+GFS; 2: OBS+GFS+OMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac059262",
   "metadata": {},
   "source": [
    "#### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d992b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tini = time.time()\n",
    "\n",
    "df_OBS   = pd.read_csv(EXTDATA_DIR + 'train/STN/' + 'trainOBS.csv')\n",
    "df_OBS_subset = df_OBS[['latitude','longitude']]\n",
    "\n",
    "df_OMI = pd.read_csv(EXTDATA_DIR + 'train/OMI/' + 'trainOMI.csv')\n",
    "df_OMI_subset = df_OMI[['NO2_OMI','NO2Tr_OMI']]\n",
    "\n",
    "df_GFS   = pd.read_csv(EXTDATA_DIR + 'train/GFS/' + 'trainGFS.csv')\n",
    "\n",
    "if trnOpt == 1:\n",
    "    df_trn   = pd.concat([df_OBS_subset,df_GFS], axis=1)\n",
    "\n",
    "    df_trn = df_trn[['latitude','longitude',\n",
    "                     'cosJDAY','sinJDAY','WDAY',\n",
    "                     'PBLH_0','PBLH_3','PBLH_6','PBLH_9','PBLH_12','PBLH_15','PBLH_18','PBLH_21',\n",
    "                     'dT_0','dT_3','dT_6','dT_9','dT_12','dT_15','dT_18','dT_21',\n",
    "                     'SHFX_0','SHFX_3','SHFX_6','SHFX_9','SHFX_12','SHFX_15','SHFX_18','SHFX_21',\n",
    "                     'M10_0','M10_3','M10_6','M10_9','M10_12','M10_15','M10_18','M10_21',\n",
    "                     'M100_0','M100_3','M100_6','M100_9','M100_12','M100_15','M100_18','M100_21',\n",
    "                     'alpha_0','alpha_3','alpha_6','alpha_9','alpha_12','alpha_15','alpha_18','alpha_21',\n",
    "                     'beta_0','beta_3','beta_6','beta_9','beta_12','beta_15','beta_18','beta_21',\n",
    "                     'cosX100_0','cosX100_3','cosX100_6','cosX100_9','cosX100_12','cosX100_15','cosX100_18','cosX100_21',\n",
    "                     'sinX100_0','sinX100_3','sinX100_6','sinX100_9','sinX100_12','sinX100_15','sinX100_18','sinX100_21',\n",
    "                     'VENT_0','VENT_3','VENT_6','VENT_9','VENT_12','VENT_15','VENT_18','VENT_21',\n",
    "                     'T2_0','T2_3','T2_6','T2_9','T2_12','T2_15','T2_18','T2_21',\n",
    "                     'RH_0','RH_3','RH_6','RH_9','RH_12','RH_15','RH_18','RH_21',\n",
    "                     'NO2']]\n",
    "        \n",
    "elif trnOpt == 2:\n",
    "    df_trn   = pd.concat([df_OBS_subset,df_OMI_subset,df_GFS], axis=1)\n",
    "\n",
    "    df_trn = df_trn[['latitude','longitude',\n",
    "                     'NO2_OMI','NO2Tr_OMI',\n",
    "                     'cosJDAY','sinJDAY','WDAY',\n",
    "                     'PBLH_0','PBLH_3','PBLH_6','PBLH_9','PBLH_12','PBLH_15','PBLH_18','PBLH_21',\n",
    "                     'dT_0','dT_3','dT_6','dT_9','dT_12','dT_15','dT_18','dT_21',\n",
    "                     'SHFX_0','SHFX_3','SHFX_6','SHFX_9','SHFX_12','SHFX_15','SHFX_18','SHFX_21',\n",
    "                     'M10_0','M10_3','M10_6','M10_9','M10_12','M10_15','M10_18','M10_21',\n",
    "                     'M100_0','M100_3','M100_6','M100_9','M100_12','M100_15','M100_18','M100_21',\n",
    "                     'alpha_0','alpha_3','alpha_6','alpha_9','alpha_12','alpha_15','alpha_18','alpha_21',\n",
    "                     'beta_0','beta_3','beta_6','beta_9','beta_12','beta_15','beta_18','beta_21',\n",
    "                     'cosX100_0','cosX100_3','cosX100_6','cosX100_9','cosX100_12','cosX100_15','cosX100_18','cosX100_21',\n",
    "                     'sinX100_0','sinX100_3','sinX100_6','sinX100_9','sinX100_12','sinX100_15','sinX100_18','sinX100_21',\n",
    "                     'VENT_0','VENT_3','VENT_6','VENT_9','VENT_12','VENT_15','VENT_18','VENT_21',\n",
    "                     'T2_0','T2_3','T2_6','T2_9','T2_12','T2_15','T2_18','T2_21',\n",
    "                     'RH_0','RH_3','RH_6','RH_9','RH_12','RH_15','RH_18','RH_21',\n",
    "                     'NO2']]\n",
    "    \n",
    "df_trn   = df_trn.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_trn['NO2'],100);\n",
    "plt.show()\n",
    "\n",
    "p10 = np.percentile(df_trn['NO2'],10)\n",
    "p50 = np.percentile(df_trn['NO2'],50)\n",
    "p90 = np.percentile(df_trn['NO2'],90)\n",
    "p95 = np.percentile(df_trn['NO2'],95)\n",
    "p99 = np.percentile(df_trn['NO2'],99)\n",
    "pmax= np.max(df_trn['NO2'])\n",
    "print((p10,p50,p90,p95,p99,pmax))\n",
    "\n",
    "df_trn.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed9eb1",
   "metadata": {},
   "source": [
    "#### Get rid of \"rare\" events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc27023",
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = np.percentile(df_trn['NO2'],99.9)\n",
    "indx = np.where(df_trn['NO2'] > thres)\n",
    "print(np.size(indx))\n",
    "df_trn = df_trn[df_trn['NO2'] < thres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrnVal  = df_trn.iloc[:,0:-1]\n",
    "yTrnVal  = df_trn.iloc[:,-1:]\n",
    "\n",
    "XTrnVal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrnVal.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37780a8",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "randSeed = np.random.randint(1000)\n",
    "\n",
    "#----------------------------\n",
    "#Coarse hyperparameter tuning\n",
    "params = {'num_leaves':2**np.arange(2,10,1),'max_depth':np.arange(1,11,1),\\\n",
    "                  'learning_rate':np.array([0.005,0.01,0.025]),\\\n",
    "                  'reg_lambda':np.arange(0,3.01,0.5),'reg_alpha':np.arange(0,3.01,0.5),\\\n",
    "                  'subsample':np.arange(0.1,1.01,0.1), 'colsample_bytree':np.arange(0.1,1.01,0.1)}\n",
    "\n",
    "lgbReg = lgb.LGBMRegressor(objective='mse', random_state = randSeed, force_col_wise = True, metric='None', \n",
    "                           first_metric_only=True, n_jobs=-1)    \n",
    "lgbReg.min_data_in_leaf = 100 #Important to avoid overfitting\n",
    "randLgbReg = HalvingRandomSearchCV(lgbReg, params, resource = 'n_estimators', min_resources = 100, \n",
    "                                   n_candidates = nTrial, max_resources = 1600, factor = 2, \n",
    "                                   scoring = 'neg_mean_squared_error',\n",
    "                                   cv = 10, return_train_score=True, random_state = randSeed, verbose = 10, n_jobs=1)\n",
    "\n",
    "randLgbReg.fit(XTrnVal, yTrnVal)\n",
    "trnScore = randLgbReg.cv_results_['mean_train_score']\n",
    "valScore = randLgbReg.cv_results_['mean_test_score']\n",
    "\n",
    "bestScore = -1*randLgbReg.best_score_\n",
    "indx = np.where(valScore == -bestScore)\n",
    "bestTrnScore = trnScore[indx]\n",
    "globalBestScore = bestScore\n",
    "\n",
    "et = time.time() - tini\n",
    "print((et,trnScore,valScore,bestTrnScore,bestScore))\n",
    "\n",
    "bestParam = randLgbReg.best_params_\n",
    "print(bestParam)\n",
    "lgbRegNew   = randLgbReg.best_estimator_\n",
    "\n",
    "dump(lgbRegNew, open(TUNING_DIR + 'CoarseLGBTuningFS_' + str(trnOpt) + '_' + str(nTrial) + '.pkl', 'wb'))\n",
    "\n",
    "#----------------------------\n",
    "#Early stopping\n",
    "\n",
    "#Refit the model with early stopping (randomly select 10% data for validation)\n",
    "nSamples    = np.size(yTrnVal)\n",
    "nSamplesVal = int(np.rint(0.1*nSamples))\n",
    "\n",
    "sumTrnScore = 0\n",
    "sumValScore = 0\n",
    "count = 0\n",
    "for n in range(100000):\n",
    "    tini = time.time()\n",
    "\n",
    "    #Refit the data with best parameters + early stopping\n",
    "    lgbRegES = lgbRegNew\n",
    "    lgbRegES.n_jobs = -1\n",
    "\n",
    "    #Overwrite n_estimators to 2000 \n",
    "    lgbRegES.n_estimators = 10000\n",
    "\n",
    "    #Pick a random start location for validation set\n",
    "    iVal        = np.random.randint(0,nSamples-nSamplesVal+1)\n",
    "\n",
    "    print((nSamples,nSamplesVal,iVal))\n",
    "\n",
    "    XValDATA    = XTrnVal.iloc[iVal:iVal+nSamplesVal+1,:]\n",
    "    yValDATA    = yTrnVal.iloc[iVal:iVal+nSamplesVal+1,:]\n",
    "    XVal        = pd.DataFrame(data=XValDATA)\n",
    "    yVal        = pd.DataFrame(data=yValDATA)\n",
    "\n",
    "    XTrn1DATA   = XTrnVal.iloc[0:iVal,:]\n",
    "    yTrn1DATA   = yTrnVal.iloc[0:iVal,:]\n",
    "\n",
    "    XTrn2DATA   = XTrnVal.iloc[iVal+nSamplesVal+1:,:]\n",
    "    yTrn2DATA   = yTrnVal.iloc[iVal+nSamplesVal+1:,:]\n",
    "\n",
    "    XTrn        = pd.concat([XTrn1DATA, XTrn2DATA], axis=0)\n",
    "    yTrn        = pd.concat([yTrn1DATA, yTrn2DATA], axis=0)\n",
    "    \n",
    "    eval_set = [(XTrn, yTrn),(XVal, yVal)]\n",
    "    \n",
    "    lgbRegES.fit(XTrn, yTrn, early_stopping_rounds=25, eval_metric=\"l2\", eval_set=eval_set, verbose=-1)\n",
    "\n",
    "    trnScore    = lgbRegES.best_score_[\"training\"][\"l2\"]\n",
    "    valScore    = lgbRegES.best_score_[\"valid_1\"][\"l2\"]\n",
    "                \n",
    "    et          = time.time() - tini \n",
    "\n",
    "    if (trnScore < globalBestScore) & (valScore < globalBestScore):\n",
    "        \n",
    "        dump(lgbRegES, open(TUNING_DIR + 'ESLGBTuningFS_' + str(trnOpt) + '_' + str(nTrial) + '_' + str(count) + '.pkl', 'wb'))            \n",
    "\n",
    "        count = count + 1\n",
    "        sumTrnScore = sumTrnScore + trnScore\n",
    "        sumValScore = sumValScore + valScore\n",
    "        print((count,et,sumTrnScore/count,sumValScore/count))\n",
    "        if count == nEns:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af9501fc74fe6e40fad117f664a0d0009d5eafaa8c2d8d12d4670dbf03338fe2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('3rd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
