{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41001e6d-7153-4135-8940-41cfbcfe38b8",
   "metadata": {},
   "source": [
    "## Processing train gfs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "262acfa1-10a4-46d7-b61c-6092aab8f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pygrib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "WORK_DIR = Path('../data/')\n",
    "SAVE_DIR = Path('../data/raw/train_gfs_data/')\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATAVOL = Path('../../../../datavol/pm25_1/train_gfs_data')\n",
    "assert WORK_DIR.exists()\n",
    "assert SAVE_DIR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5708e228-ad27-4e95-8ed3-a75d9f62cc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>location</th>\n",
       "      <th>tz</th>\n",
       "      <th>wkt</th>\n",
       "      <th>elevation_mean</th>\n",
       "      <th>elevation_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1X116</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>Asia/Taipei</td>\n",
       "      <td>POLYGON ((121.5257644471362 24.97766123020391,...</td>\n",
       "      <td>23.371277</td>\n",
       "      <td>87.977536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1Z2W7</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Asia/Calcutta</td>\n",
       "      <td>POLYGON ((77.30453178416276 28.54664454217707,...</td>\n",
       "      <td>209.248475</td>\n",
       "      <td>17.782785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3S31A</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "      <td>Etc/GMT+8</td>\n",
       "      <td>POLYGON ((-117.9338248256995 33.79558357488509...</td>\n",
       "      <td>236.640772</td>\n",
       "      <td>248.347388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6EIL6</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Asia/Calcutta</td>\n",
       "      <td>POLYGON ((77.07995296313287 28.54664454217707,...</td>\n",
       "      <td>209.248475</td>\n",
       "      <td>17.782785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7334C</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Asia/Calcutta</td>\n",
       "      <td>POLYGON ((77.12486872733885 28.54664454217707,...</td>\n",
       "      <td>209.248475</td>\n",
       "      <td>17.782785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grid_id             location             tz  \\\n",
       "0   1X116               Taipei    Asia/Taipei   \n",
       "1   1Z2W7                Delhi  Asia/Calcutta   \n",
       "2   3S31A  Los Angeles (SoCAB)      Etc/GMT+8   \n",
       "3   6EIL6                Delhi  Asia/Calcutta   \n",
       "4   7334C                Delhi  Asia/Calcutta   \n",
       "\n",
       "                                                 wkt  elevation_mean  \\\n",
       "0  POLYGON ((121.5257644471362 24.97766123020391,...       23.371277   \n",
       "1  POLYGON ((77.30453178416276 28.54664454217707,...      209.248475   \n",
       "2  POLYGON ((-117.9338248256995 33.79558357488509...      236.640772   \n",
       "3  POLYGON ((77.07995296313287 28.54664454217707,...      209.248475   \n",
       "4  POLYGON ((77.12486872733885 28.54664454217707,...      209.248475   \n",
       "\n",
       "   elevation_var  \n",
       "0      87.977536  \n",
       "1      17.782785  \n",
       "2     248.347388  \n",
       "3      17.782785  \n",
       "4      17.782785  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_data = pd.read_csv(WORK_DIR / 'grid_metadata.csv')\n",
    "grid_to_location = {g: l for g, l in zip(grid_data['grid_id'].values, grid_data['location'].values)}\n",
    "grid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89a87829-f31d-4c86-859f-60194349f3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>value</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>3S31A</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>A2FBI</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>DJN0F</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>E5P9N</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>FRITQ</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34307</th>\n",
       "      <td>2020-12-31T18:30:00Z</td>\n",
       "      <td>P8JA5</td>\n",
       "      <td>368.611111</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34308</th>\n",
       "      <td>2020-12-31T18:30:00Z</td>\n",
       "      <td>PW0JT</td>\n",
       "      <td>294.425000</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34309</th>\n",
       "      <td>2020-12-31T18:30:00Z</td>\n",
       "      <td>VXNN3</td>\n",
       "      <td>224.857143</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34310</th>\n",
       "      <td>2020-12-31T18:30:00Z</td>\n",
       "      <td>VYH7U</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34311</th>\n",
       "      <td>2020-12-31T18:30:00Z</td>\n",
       "      <td>ZF3ZW</td>\n",
       "      <td>410.500000</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34312 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime grid_id       value             location\n",
       "0      2018-02-01T08:00:00Z   3S31A   11.400000  Los Angeles (SoCAB)\n",
       "1      2018-02-01T08:00:00Z   A2FBI   17.000000  Los Angeles (SoCAB)\n",
       "2      2018-02-01T08:00:00Z   DJN0F   11.100000  Los Angeles (SoCAB)\n",
       "3      2018-02-01T08:00:00Z   E5P9N   22.100000  Los Angeles (SoCAB)\n",
       "4      2018-02-01T08:00:00Z   FRITQ   29.800000  Los Angeles (SoCAB)\n",
       "...                     ...     ...         ...                  ...\n",
       "34307  2020-12-31T18:30:00Z   P8JA5  368.611111                Delhi\n",
       "34308  2020-12-31T18:30:00Z   PW0JT  294.425000                Delhi\n",
       "34309  2020-12-31T18:30:00Z   VXNN3  224.857143                Delhi\n",
       "34310  2020-12-31T18:30:00Z   VYH7U  287.000000                Delhi\n",
       "34311  2020-12-31T18:30:00Z   ZF3ZW  410.500000                Delhi\n",
       "\n",
       "[34312 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(WORK_DIR / 'train_labels.csv')\n",
    "train_data['location'] = train_data['grid_id'].apply(lambda x: grid_to_location[x])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc31aeb4-cc75-4d0b-885f-0b3a33f9f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping duplicates: (34312, 2)\n",
      "After dropping duplicates: (2541, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data[['datetime', 'location']]\n",
    "print(f\"Before dropping duplicates: {train_data.shape}\")\n",
    "train_data = train_data.drop_duplicates()\n",
    "print(f\"After dropping duplicates: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39456b5e-9a38-45e2-8a35-f9d583a78caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_status(filepath, filesize):\n",
    "    # sys.stdout.write('\\r')\n",
    "    # sys.stdout.flush()\n",
    "    size = int(os.stat(filepath).st_size)\n",
    "    percent_complete = (size/filesize)*100\n",
    "    # sys.stdout.write('%.3f %s' % (percent_complete, '% Completed'))\n",
    "    # sys.stdout.flush()\n",
    "\n",
    "# Write your email and password of ncar account\n",
    "email = \"isha@drivendata.org\"\n",
    "pswd = \"XY.t5Vvj.6g5ztP\"\n",
    "\n",
    "url = 'https://rda.ucar.edu/cgi-bin/login'\n",
    "values = {'email' : email, 'passwd' : pswd, 'action' : 'login'}\n",
    "# Authenticate\n",
    "ret = requests.post(url,data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)\n",
    "DSPATH = 'https://rda.ucar.edu/data/ds084.1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b714b757-d8e4-401a-8288-7b56186421fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = {\n",
    "    'Los Angeles (SoCAB)': 'la',\n",
    "    'Taipei': 'tpe', \n",
    "    'Delhi': 'dl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52e6cbb7-c8b5-4b1b-a038-a72b3b3e9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_cycle(dt):\n",
    "    hour = dt.hour\n",
    "    ans = None\n",
    "    for h in [0, 6, 12, 18]:\n",
    "        if hour >= h:\n",
    "            ans = h\n",
    "    return ans\n",
    "    \n",
    "def get_nearest_forecast(dt):\n",
    "    cycle = get_nearest_cycle(dt)\n",
    "    available_forecasts = [0, 3, 6, 9, 12, 15, 18, 21]\n",
    "    forecast = 0\n",
    "    for f in available_forecasts:\n",
    "        if cycle + f > dt.hour:\n",
    "            break\n",
    "        forecast = f\n",
    "    return \"{:02d}\".format(cycle), \"{:03d}\".format(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dd4b40c-0feb-43e0-90a6-526c2af8af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2541/2541 [12:18:25<00:00, 17.44s/it]\n"
     ]
    }
   ],
   "source": [
    "indices = [idx for idx in range(len(train_data))]\n",
    "variables = ['Wind speed (gust)', 'Haines Index', 'Surface pressure', 'Orography', 'Temperature', 'Water equivalent of accumulated snow depth (deprecated)', 'Snow depth', 'Potential evaporation rate', 'Percent frozen precipitation', 'Convective precipitation rate', 'Precipitation rate', 'Total Precipitation', 'Convective precipitation (water)', 'Water runoff', 'Categorical snow', 'Categorical ice pellets', 'Categorical freezing rain', 'Categorical rain', 'Latent heat net flux', 'Sensible heat net flux', 'Ground heat flux', 'Momentum flux, u component', 'Momentum flux, v component', 'Zonal flux of gravity wave stress', 'Meridional flux of gravity wave stress', 'Wilting Point', 'Field Capacity', 'Sunshine Duration', 'Surface lifted index', 'Convective available potential energy', 'Convective inhibition', 'Downward short-wave radiation flux', 'Downward long-wave radiation flux', 'Upward short-wave radiation flux', 'Upward long-wave radiation flux', 'Best (4-layer) lifted index', 'Planetary boundary layer height', 'Land-sea mask', 'Sea ice area fraction', 'Albedo', 'Land-sea coverage (nearest neighbor) [land=1,sea=0]']\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    dt = datetime.datetime.strptime(el['datetime'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "    loc = location_map[el['location']]\n",
    "    \n",
    "    outpath = DATAVOL / f\"{el['datetime']}_{loc}\" # EDIT - match the underscore style\n",
    "    # if not os.path.exists(outpath):\n",
    "    #     continue\n",
    "\n",
    "    cycle, forecast = get_nearest_forecast(dt)\n",
    "    year = dt.year\n",
    "    month = \"{:02d}\".format(dt.month)\n",
    "    day = \"{:02d}\".format(dt.day)\n",
    "    filename = f'{year}/{year}{month}{day}/gfs.0p25.{year}{month}{day}{cycle}.f{forecast}.grib2'\n",
    "    # print(f\"{dt} => {filename}\")\n",
    "\n",
    "    filename = DSPATH + filename\n",
    "    file_base = os.path.basename(filename)\n",
    "\n",
    "    req = requests.get(filename, cookies = ret.cookies, allow_redirects=True, stream=True)\n",
    "    filesize = int(req.headers['Content-length'])\n",
    "    with open(file_base, 'wb') as outfile:\n",
    "        chunk_size=1048576\n",
    "        for chunk in req.iter_content(chunk_size=chunk_size):\n",
    "            outfile.write(chunk)\n",
    "            if chunk_size < filesize:\n",
    "                check_file_status(file_base, filesize)\n",
    "    check_file_status(file_base, filesize)\n",
    "\n",
    "    gr = pygrib.open(file_base)\n",
    "    assets = {}\n",
    "\n",
    "    for g in gr:\n",
    "        if g.name in variables and g.typeOfLevel == 'surface':\n",
    "            assets[g.name] = g.values\n",
    "\n",
    "    assets['latitude'] = g.latlons()[0]\n",
    "    assets['longitude'] = g.latlons()[1]\n",
    "\n",
    "    np.savez_compressed(outpath, **assets)\n",
    "    !rm {file_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdb57b6f-6ce9-4e30-90fe-eaf7352827dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely\n",
      "  Downloading Shapely-1.8.1.post1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 28.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: shapely\n",
      "Successfully installed shapely-1.8.1.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2feecba-17df-4645-be0c-68492273c396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5291f9bb-7f58-46e2-9683-a4d1e75f4b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████▏                                          | 15925/34312 [2:39:08<2:52:57,  1.77it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 75%|███████████████████████████████████████████████████████████▋                    | 25585/34312 [4:23:03<1:37:34,  1.49it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34312/34312 [5:58:13<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, Polygon\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path('/home/ubuntu/datavol/pm25_1/train_gfs_data')\n",
    "assert DATA_DIR.exists(), \"{} does not exist...\".format(DATA_DIR)\n",
    "REQUIRED_BANDS = [\n",
    "    'Wind speed (gust)', 'Haines Index', 'Surface pressure', \n",
    "    'Orography', 'Temperature', 'Water equivalent of accumulated snow depth (deprecated)', \n",
    "    'Snow depth', 'Percent frozen precipitation', 'Wilting Point', 'Field Capacity',\n",
    "    'Sunshine Duration', 'Surface lifted index', 'Convective available potential energy',\n",
    "    'Convective inhibition', 'Best (4-layer) lifted index', 'Planetary boundary layer height',\n",
    "    'Land-sea mask', 'Sea ice area fraction', 'Land-sea coverage (nearest neighbor) [land=1,sea=0]',\n",
    "    'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "def floor(x, n=0):\n",
    "    return np.floor(x * 10**n) / 10**n\n",
    "\n",
    "def ceil(x, n=0):\n",
    "    return np.ceil(x * 10**n) / 10**n\n",
    "\n",
    "def get_bounds(geometry):\n",
    "    geometry = np.array(geometry)\n",
    "    long = [\n",
    "        floor(geometry[:, 0].min(), 1),\n",
    "        ceil(geometry[:, 0].max(), 1)\n",
    "    ]\n",
    "    lat = [\n",
    "        floor(geometry[:, 1].min(), 1),\n",
    "        ceil(geometry[:, 1].max(), 1)\n",
    "    ]\n",
    "    return long, lat\n",
    "\n",
    "def round_off(point, res):\n",
    "    spread = np.arange(np.floor(point), np.ceil(point) + 1, res)\n",
    "    adiff = np.abs(spread - point)\n",
    "    return spread[np.argmin(adiff)]\n",
    "\n",
    "def get_boundary(geometry, res):\n",
    "    long = np.array(geometry)[:, 0]\n",
    "    lat = np.array(geometry)[:, 1]\n",
    "    \n",
    "    min_lat, max_lat = lat.min(), lat.max()\n",
    "    min_long, max_long = long.min(), long.max()\n",
    "    \n",
    "    min_lat = round_off(min_lat - res / 2, res)\n",
    "    max_lat = round_off(max_lat + res / 2, res)\n",
    "    min_long = round_off(min_long - res / 2, res)\n",
    "    max_long = round_off(max_long + res / 2, res)\n",
    "    \n",
    "    return [min_long, max_long], [min_lat, max_lat]\n",
    "\n",
    "def mask(filename, geometry):\n",
    "    data = np.load(filename)\n",
    "    assets = {}\n",
    "    for key in data.keys():\n",
    "        assets[key] = data[key].ravel()\n",
    "    # longb, latb = get_bounds(geometry)\n",
    "    longb, latb = get_boundary(geometry, 0.25)\n",
    "    latitude = assets['latitude']\n",
    "    longitude = assets['longitude']\n",
    "    indices = (latitude >= latb[0]) & (latitude <= latb[1]) & (longitude >= longb[0]) & (longitude <= longb[1])\n",
    "\n",
    "    new_ass = {}\n",
    "    for k in assets.keys():\n",
    "        new_ass[k] = assets[k][indices]\n",
    "                                                                   \n",
    "    df = pd.DataFrame(new_ass)\n",
    "    return df\n",
    "\n",
    "train_data = pd.read_csv(\"../data/train_labels.csv\")\n",
    "grid_data = pd.read_csv(\"../data/grid_metadata.csv\")\n",
    "location_map = {\n",
    "    'Los Angeles (SoCAB)': 'la',\n",
    "    'Taipei': 'tpe', \n",
    "    'Delhi': 'dl'\n",
    "}\n",
    "\n",
    "indices = [idx for idx in range(len(train_data))]\n",
    "total_data = defaultdict(lambda: [])\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    grid_id = el['grid_id']\n",
    "    dt = el['datetime']\n",
    "\n",
    "    grid_el = grid_data.loc[grid_data['grid_id'] == grid_id]\n",
    "    loc = location_map[grid_el['location'].iloc[0]]\n",
    "    \n",
    "    geometry = grid_el['wkt'].values[0]\n",
    "    geometry = geometry.replace('(', '', -1)\n",
    "    geometry = geometry.replace(')', '', -1)\n",
    "    geometry = geometry.replace(',', '', -1)\n",
    "    geometry = list(map(float, geometry.split()[1:]))\n",
    "    geometry = [geometry[i:i+2] for i in range(0, len(geometry), 2)]\n",
    "\n",
    "    filename = '{}_{}.npz'.format(dt, loc)\n",
    "    # filename = filename.replace(':', '_', -1) # EDIT - no longer needed, looks like colons are not replaced when writing\n",
    "    filename = DATA_DIR / filename\n",
    "    try:\n",
    "        cur_data = mask(filename, geometry)\n",
    "    except Exception as e:\n",
    "        cur_data = pd.DataFrame({})\n",
    "        print(f\"{idx}: {e}\")\n",
    "\n",
    "    for key in REQUIRED_BANDS:\n",
    "        try:\n",
    "            _band = cur_data[key].values\n",
    "            _band = np.concatenate((\n",
    "                _band[_band <= 0], _band[_band > 0]\n",
    "            ))\n",
    "        except KeyError:\n",
    "            _band = np.array([np.nan, np.nan])\n",
    "        total_data[f\"{key}_mean\"].append(_band.mean())\n",
    "        total_data[f\"{key}_var\"].append(_band.std() ** 2)\n",
    "\n",
    "total_data = pd.DataFrame(total_data)\n",
    "total_data.to_csv('/home/ubuntu/datavol/pm25_1/train_gfs_data/train_gfs_newrun.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cf88a-5844-4327-93bd-04fc78ebad26",
   "metadata": {},
   "source": [
    "## Processing test gfs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3081c7c-c1b3-49d1-8a08-04237ecc0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pygrib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "WORK_DIR = Path('../data/')\n",
    "SAVE_DIR = Path('../data/raw/test_gfs_data/')\n",
    "!mkdir {SAVE_DIR}\n",
    "WORK_DIR.exists()\n",
    "\n",
    "DATAVOL = Path('../../../../datavol/pm25_1/test_gfs_data')\n",
    "!mkdir {DATAVOL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81bcdcc1-6399-43c9-a348-7ad2fa8f5b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>location</th>\n",
       "      <th>tz</th>\n",
       "      <th>wkt</th>\n",
       "      <th>elevation_mean</th>\n",
       "      <th>elevation_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1X116</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>Asia/Taipei</td>\n",
       "      <td>POLYGON ((121.5257644471362 24.97766123020391,...</td>\n",
       "      <td>23.371277</td>\n",
       "      <td>87.977536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1Z2W7</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Asia/Calcutta</td>\n",
       "      <td>POLYGON ((77.30453178416276 28.54664454217707,...</td>\n",
       "      <td>209.248475</td>\n",
       "      <td>17.782785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3S31A</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "      <td>Etc/GMT+8</td>\n",
       "      <td>POLYGON ((-117.9338248256995 33.79558357488509...</td>\n",
       "      <td>236.640772</td>\n",
       "      <td>248.347388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6EIL6</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Asia/Calcutta</td>\n",
       "      <td>POLYGON ((77.07995296313287 28.54664454217707,...</td>\n",
       "      <td>209.248475</td>\n",
       "      <td>17.782785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7334C</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Asia/Calcutta</td>\n",
       "      <td>POLYGON ((77.12486872733885 28.54664454217707,...</td>\n",
       "      <td>209.248475</td>\n",
       "      <td>17.782785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grid_id             location             tz  \\\n",
       "0   1X116               Taipei    Asia/Taipei   \n",
       "1   1Z2W7                Delhi  Asia/Calcutta   \n",
       "2   3S31A  Los Angeles (SoCAB)      Etc/GMT+8   \n",
       "3   6EIL6                Delhi  Asia/Calcutta   \n",
       "4   7334C                Delhi  Asia/Calcutta   \n",
       "\n",
       "                                                 wkt  elevation_mean  \\\n",
       "0  POLYGON ((121.5257644471362 24.97766123020391,...       23.371277   \n",
       "1  POLYGON ((77.30453178416276 28.54664454217707,...      209.248475   \n",
       "2  POLYGON ((-117.9338248256995 33.79558357488509...      236.640772   \n",
       "3  POLYGON ((77.07995296313287 28.54664454217707,...      209.248475   \n",
       "4  POLYGON ((77.12486872733885 28.54664454217707,...      209.248475   \n",
       "\n",
       "   elevation_var  \n",
       "0      87.977536  \n",
       "1      17.782785  \n",
       "2     248.347388  \n",
       "3      17.782785  \n",
       "4      17.782785  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_data = pd.read_csv(WORK_DIR / 'grid_metadata.csv')\n",
    "grid_to_location = {g: l for g, l in zip(grid_data['grid_id'].values, grid_data['location'].values)}\n",
    "grid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dc720cb-a2f0-4e1d-9a08-3ab84af32318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>value</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-07T16:00:00Z</td>\n",
       "      <td>1X116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Taipei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-07T16:00:00Z</td>\n",
       "      <td>9Q6TA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Taipei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-07T16:00:00Z</td>\n",
       "      <td>KW43U</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Taipei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-07T16:00:00Z</td>\n",
       "      <td>VR4WG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Taipei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-07T16:00:00Z</td>\n",
       "      <td>XJF9O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Taipei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13499</th>\n",
       "      <td>2021-08-24T08:00:00Z</td>\n",
       "      <td>QJHW4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>2021-08-24T08:00:00Z</td>\n",
       "      <td>VBLD0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13501</th>\n",
       "      <td>2021-08-24T08:00:00Z</td>\n",
       "      <td>WT52R</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13502</th>\n",
       "      <td>2021-08-24T08:00:00Z</td>\n",
       "      <td>ZP1FZ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13503</th>\n",
       "      <td>2021-08-24T08:00:00Z</td>\n",
       "      <td>ZZ8JF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Los Angeles (SoCAB)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13504 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime grid_id  value             location\n",
       "0      2017-01-07T16:00:00Z   1X116    0.0               Taipei\n",
       "1      2017-01-07T16:00:00Z   9Q6TA    0.0               Taipei\n",
       "2      2017-01-07T16:00:00Z   KW43U    0.0               Taipei\n",
       "3      2017-01-07T16:00:00Z   VR4WG    0.0               Taipei\n",
       "4      2017-01-07T16:00:00Z   XJF9O    0.0               Taipei\n",
       "...                     ...     ...    ...                  ...\n",
       "13499  2021-08-24T08:00:00Z   QJHW4    0.0  Los Angeles (SoCAB)\n",
       "13500  2021-08-24T08:00:00Z   VBLD0    0.0  Los Angeles (SoCAB)\n",
       "13501  2021-08-24T08:00:00Z   WT52R    0.0  Los Angeles (SoCAB)\n",
       "13502  2021-08-24T08:00:00Z   ZP1FZ    0.0  Los Angeles (SoCAB)\n",
       "13503  2021-08-24T08:00:00Z   ZZ8JF    0.0  Los Angeles (SoCAB)\n",
       "\n",
       "[13504 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(WORK_DIR / 'submission_format.csv')\n",
    "train_data['location'] = train_data['grid_id'].apply(lambda x: grid_to_location[x])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4ef50c2-ccfe-4821-b2b0-b17cd3b0632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping duplicates: (13504, 2)\n",
      "After dropping duplicates: (1256, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data[['datetime', 'location']]\n",
    "print(f\"Before dropping duplicates: {train_data.shape}\")\n",
    "train_data = train_data.drop_duplicates()\n",
    "print(f\"After dropping duplicates: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6326017c-2907-41e8-8257-0eec01910ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_status(filepath, filesize):\n",
    "    # sys.stdout.write('\\r')\n",
    "    # sys.stdout.flush()\n",
    "    size = int(os.stat(filepath).st_size)\n",
    "    percent_complete = (size/filesize)*100\n",
    "    # sys.stdout.write('%.3f %s' % (percent_complete, '% Completed'))\n",
    "    # sys.stdout.flush()\n",
    "\n",
    "# Write your email and password of ncar account\n",
    "email = \"isha@drivendata.org\"\n",
    "pswd = \"XY.t5Vvj.6g5ztP\"\n",
    "\n",
    "url = 'https://rda.ucar.edu/cgi-bin/login'\n",
    "values = {'email' : email, 'passwd' : pswd, 'action' : 'login'}\n",
    "# Authenticate\n",
    "ret = requests.post(url,data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)\n",
    "DSPATH = 'https://rda.ucar.edu/data/ds084.1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7e18c77-abb1-4389-bcd6-a786ae6057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = {\n",
    "    'Los Angeles (SoCAB)': 'la',\n",
    "    'Taipei': 'tpe', \n",
    "    'Delhi': 'dl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19d4509e-dd09-4443-a0eb-4c0e55d32a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_cycle(dt):\n",
    "    hour = dt.hour\n",
    "    ans = None\n",
    "    for h in [0, 6, 12, 18]:\n",
    "        if hour >= h:\n",
    "            ans = h\n",
    "    return ans\n",
    "    \n",
    "def get_nearest_forecast(dt):\n",
    "    cycle = get_nearest_cycle(dt)\n",
    "    available_forecasts = [0, 3, 6, 9, 12, 15, 18, 21]\n",
    "    forecast = 0\n",
    "    for f in available_forecasts:\n",
    "        if cycle + f > dt.hour:\n",
    "            break\n",
    "        forecast = f\n",
    "    return \"{:02d}\".format(cycle), \"{:03d}\".format(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af8c9504-9698-47e1-989c-a289182a1ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1256/1256 [6:14:51<00:00, 17.91s/it]\n"
     ]
    }
   ],
   "source": [
    "indices = [idx for idx in range(len(train_data))]\n",
    "variables = ['Wind speed (gust)', 'Haines Index', 'Surface pressure', 'Orography', 'Temperature', 'Water equivalent of accumulated snow depth (deprecated)', 'Snow depth', 'Potential evaporation rate', 'Percent frozen precipitation', 'Convective precipitation rate', 'Precipitation rate', 'Total Precipitation', 'Convective precipitation (water)', 'Water runoff', 'Categorical snow', 'Categorical ice pellets', 'Categorical freezing rain', 'Categorical rain', 'Latent heat net flux', 'Sensible heat net flux', 'Ground heat flux', 'Momentum flux, u component', 'Momentum flux, v component', 'Zonal flux of gravity wave stress', 'Meridional flux of gravity wave stress', 'Wilting Point', 'Field Capacity', 'Sunshine Duration', 'Surface lifted index', 'Convective available potential energy', 'Convective inhibition', 'Downward short-wave radiation flux', 'Downward long-wave radiation flux', 'Upward short-wave radiation flux', 'Upward long-wave radiation flux', 'Best (4-layer) lifted index', 'Planetary boundary layer height', 'Land-sea mask', 'Sea ice area fraction', 'Albedo', 'Land-sea coverage (nearest neighbor) [land=1,sea=0]']\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    dt = datetime.datetime.strptime(el['datetime'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "    loc = location_map[el['location']]\n",
    "\n",
    "    cycle, forecast = get_nearest_forecast(dt)\n",
    "    year = dt.year\n",
    "    month = \"{:02d}\".format(dt.month)\n",
    "    day = \"{:02d}\".format(dt.day)\n",
    "    filename = f'{year}/{year}{month}{day}/gfs.0p25.{year}{month}{day}{cycle}.f{forecast}.grib2'\n",
    "    # print(f\"{dt} => {filename}\")\n",
    "\n",
    "    filename = DSPATH + filename\n",
    "    file_base = os.path.basename(filename)\n",
    "\n",
    "    req = requests.get(filename, cookies = ret.cookies, allow_redirects=True, stream=True)\n",
    "    filesize = int(req.headers['Content-length'])\n",
    "    with open(file_base, 'wb') as outfile:\n",
    "        chunk_size=1048576\n",
    "        for chunk in req.iter_content(chunk_size=chunk_size):\n",
    "            outfile.write(chunk)\n",
    "            if chunk_size < filesize:\n",
    "                check_file_status(file_base, filesize)\n",
    "    check_file_status(file_base, filesize)\n",
    "\n",
    "    gr = pygrib.open(file_base)\n",
    "    assets = {}\n",
    "\n",
    "    for g in gr:\n",
    "        if g.name in variables and g.typeOfLevel == 'surface':\n",
    "            assets[g.name] = g.values\n",
    "\n",
    "    assets['latitude'] = g.latlons()[0]\n",
    "    assets['longitude'] = g.latlons()[1]\n",
    "    \n",
    "    outpath = DATAVOL / f\"{el['datetime']}_{loc}\" # EDIT - match the underscore style\n",
    "\n",
    "    np.savez_compressed(outpath, **assets)\n",
    "    !rm {file_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c73c5207-8dcb-40b8-a9f1-ab47777b4b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 13504/13504 [2:17:20<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, Polygon\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path('../data/raw/test_gfs_data/')\n",
    "assert DATA_DIR.exists(), \"{} does not exist...\".format(DATA_DIR)\n",
    "REQUIRED_BANDS = [\n",
    "    'Wind speed (gust)', 'Haines Index', 'Surface pressure', \n",
    "    'Orography', 'Temperature', 'Water equivalent of accumulated snow depth (deprecated)', \n",
    "    'Snow depth', 'Percent frozen precipitation', 'Wilting Point', 'Field Capacity',\n",
    "    'Sunshine Duration', 'Surface lifted index', 'Convective available potential energy',\n",
    "    'Convective inhibition', 'Best (4-layer) lifted index', 'Planetary boundary layer height',\n",
    "    'Land-sea mask', 'Sea ice area fraction', 'Land-sea coverage (nearest neighbor) [land=1,sea=0]',\n",
    "    'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "def floor(x, n=0):\n",
    "    return np.floor(x * 10**n) / 10**n\n",
    "\n",
    "def ceil(x, n=0):\n",
    "    return np.ceil(x * 10**n) / 10**n\n",
    "\n",
    "def get_bounds(geometry):\n",
    "    geometry = np.array(geometry)\n",
    "    long = [\n",
    "        floor(geometry[:, 0].min(), 1),\n",
    "        ceil(geometry[:, 0].max(), 1)\n",
    "    ]\n",
    "    lat = [\n",
    "        floor(geometry[:, 1].min(), 1),\n",
    "        ceil(geometry[:, 1].max(), 1)\n",
    "    ]\n",
    "    return long, lat\n",
    "\n",
    "def round_off(point, res):\n",
    "    spread = np.arange(np.floor(point), np.ceil(point) + 1, res)\n",
    "    adiff = np.abs(spread - point)\n",
    "    return spread[np.argmin(adiff)]\n",
    "\n",
    "def get_boundary(geometry, res):\n",
    "    long = np.array(geometry)[:, 0]\n",
    "    lat = np.array(geometry)[:, 1]\n",
    "    \n",
    "    min_lat, max_lat = lat.min(), lat.max()\n",
    "    min_long, max_long = long.min(), long.max()\n",
    "    \n",
    "    min_lat = round_off(min_lat - res / 2, res)\n",
    "    max_lat = round_off(max_lat + res / 2, res)\n",
    "    min_long = round_off(min_long - res / 2, res)\n",
    "    max_long = round_off(max_long + res / 2, res)\n",
    "    \n",
    "    return [min_long, max_long], [min_lat, max_lat]\n",
    "\n",
    "def mask(filename, geometry):\n",
    "    data = np.load(filename)\n",
    "    assets = {}\n",
    "    for key in data.keys():\n",
    "        assets[key] = data[key].ravel()\n",
    "    # longb, latb = get_bounds(geometry)\n",
    "    longb, latb = get_boundary(geometry, 0.25)\n",
    "    latitude = assets['latitude']\n",
    "    longitude = assets['longitude']\n",
    "    indices = (latitude >= latb[0]) & (latitude <= latb[1]) & (longitude >= longb[0]) & (longitude <= longb[1])\n",
    "\n",
    "    new_ass = {}\n",
    "    for k in assets.keys():\n",
    "        new_ass[k] = assets[k][indices]\n",
    "                                                                   \n",
    "    df = pd.DataFrame(new_ass)\n",
    "    return df\n",
    "\n",
    "train_data = pd.read_csv(\"../data/submission_format.csv\")\n",
    "grid_data = pd.read_csv(\"../data/grid_metadata.csv\")\n",
    "location_map = {\n",
    "    'Los Angeles (SoCAB)': 'la',\n",
    "    'Taipei': 'tpe', \n",
    "    'Delhi': 'dl'\n",
    "}\n",
    "\n",
    "indices = [idx for idx in range(len(train_data))]\n",
    "total_data = defaultdict(lambda: [])\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    grid_id = el['grid_id']\n",
    "    dt = el['datetime']\n",
    "\n",
    "    grid_el = grid_data.loc[grid_data['grid_id'] == grid_id]\n",
    "    loc = location_map[grid_el['location'].iloc[0]]\n",
    "    \n",
    "    geometry = grid_el['wkt'].values[0]\n",
    "    geometry = geometry.replace('(', '', -1)\n",
    "    geometry = geometry.replace(')', '', -1)\n",
    "    geometry = geometry.replace(',', '', -1)\n",
    "    geometry = list(map(float, geometry.split()[1:]))\n",
    "    geometry = [geometry[i:i+2] for i in range(0, len(geometry), 2)]\n",
    "\n",
    "    filename = '{}_{}.npz'.format(dt, loc)\n",
    "    # outpath = DATAVOL / f\"{el['datetime']}_{loc}\"\n",
    "    # filename = filename.replace(':', '_', -1)\n",
    "    filename = DATAVOL / filename # was originally:: filename = DATA_DIR / filename\n",
    "    try:\n",
    "        cur_data = mask(filename, geometry)\n",
    "    except Exception as e:\n",
    "        cur_data = pd.DataFrame({})\n",
    "        print(f\"{idx}: {e}\")\n",
    "\n",
    "    for key in REQUIRED_BANDS:\n",
    "        try:\n",
    "            _band = cur_data[key].values\n",
    "            _band = np.concatenate((\n",
    "                _band[_band <= 0], _band[_band > 0]\n",
    "            ))\n",
    "        except KeyError:\n",
    "            _band = np.array([np.nan, np.nan])\n",
    "        total_data[f\"{key}_mean\"].append(_band.mean())\n",
    "        total_data[f\"{key}_var\"].append(_band.std() ** 2)\n",
    "\n",
    "total_data = pd.DataFrame(total_data)\n",
    "total_data.to_csv('/home/ubuntu/datavol/pm25_1/test_gfs_data/test_gfs_newrun.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f87058-52b0-41a1-b31e-03ec5b2f72e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
